<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:batch="http://www.springframework.org/schema/batch"
	xmlns:task="http://www.springframework.org/schema/task"
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:p="http://www.springframework.org/schema/p"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-4.1.xsd">

    <context:annotation-config />
    <context:component-scan base-package="jobs" />

	<bean id="myAsyncUncaughtExceptionHandler" class="exceptions.MyAsyncUncaughtExceptionHandler" />

	<task:annotation-driven executor="batchTaskExecutor" scheduler="scheduling" exception-handler="myAsyncUncaughtExceptionHandler" />

	<!-- By default when specifying @Async on a method, the executor that will be used is the one supplied to the 'annotation-driven' -->
	<!-- the value attribute of the @Async annotation can be used when needing to indicate that an executor other than the default should be used -->
	<!-- set up a default executor -->
	<!-- <task:executor id="batchTaskExecutor" pool-size="07" queue-capacity="10" rejection-policy="CALLER_RUNS" keep-alive="3600" /> -->
	<!-- <task:executor id="batchTaskExecutor" pool-size="05" queue-capacity="05" rejection-policy="CALLER_RUNS" keep-alive="1800" /> -->
	<task:executor id="batchTaskExecutor" pool-size="03" queue-capacity="05" rejection-policy="CALLER_RUNS" keep-alive="1800" />
	<!-- <task:executor id="executorWithPoolSizeRange" pool-size="5-25" queue-capacity="100" rejection-policy="DISCARD" keep-alive="3600" /> -->
	<task:scheduler id="scheduling" pool-size="05" />

	<task:scheduled-tasks scheduler="scheduling">
		<!-- new CronTrigger("* 15 9-17 * * MON-FRI") -->
		<!-- java.lang.IllegalArgumentException: Cron expression must consist of 6 fields (found 7 in "0 00 0 1 APR,MAY,JUN ? 2018") -->

		<!-- <task:scheduled ref="jobs" method="run_database_maintenance" cron="0 30 01 ? * *"  /> -->
  		<!-- <task:scheduled ref="jobs" method="import_hvcs" cron="0 00 04 ? * *" /> -->
  		<!-- <task:scheduled ref="jobs" method="import_hvcs" cron="0 04/12 * ? * *" /> -->

		<!-- <task:scheduled ref="jobs" method="notify_wishes" cron="0 00 08 ? * *" /> -->
		<task:scheduled ref="jobs" method="notify_wishes" cron="0 2/12 * ? * *" />

  		<!-- <task:scheduled ref="jobs" method="clear_ussd" fixed-delay="900000" /> -->
  		<task:scheduled ref="jobs" method="clear_ussd" fixed-delay="720000" />

  		<!-- <task:scheduled ref="jobs" method="clear_sharing" cron="0 00 0 1 APR,MAY,JUN ?"  /> -->
  		<!-- <task:scheduled ref="jobs" method="setDefaultBonus" cron="00 00 12 ? * *" /> -->
  		<task:scheduled ref="jobs" method="setDefaultBonus" cron="00 0/12 * ? * *" />
	</task:scheduled-tasks>

    <bean id="transactionManager" class="org.springframework.batch.support.transaction.ResourcelessTransactionManager" />
    <bean id="jobRepository" class="org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean" p:transactionManager-ref="transactionManager" />
    <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository" p:taskExecutor-ref="batchTaskExecutor" />
 	<!-- <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository">
	  	<property name="taskExecutor">
	    	<bean class="org.springframework.core.task.SimpleAsyncTaskExecutor" />
	  	</property>
  	</bean> -->

	<batch:job id="cleanExpiredUssdRequestJob">
	  <batch:step id="cleanUssdRequest">
	  	<!-- <batch:tasklet ref="cleanExpiredUssdRequestTasklet" transaction-manager="transactionManager" /> -->
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.CleanExpiredUssdRequestAndMonitoringTasklet">
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
	  	</batch:tasklet>
	  </batch:step>
	</batch:job>

	<batch:job id="importHVConsumersJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="importHVConsumers">
	  	<!-- <batch:tasklet ref="importHVConsumersTasklet" transaction-manager="transactionManager"> -->
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.ImportHVConsumersTasklet">
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	    <batch:listener ref="jobRunListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job>

	<!-- To disable restart, set the attribute restartable to false on the job element:
	Remember that jobs are restartable by default. If you’re worried that you’ll forget that, set the restartable flag explicitly on all your jobs. -->
	<batch:job id="happyBirthdayWishesJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="wishesNotificationThroughSms">
	  	<batch:tasklet>
	      <!-- <batch:chunk commit-interval="100"> -->
	      <batch:chunk commit-interval="50">
	      <!-- <batch:chunk commit-interval="30"> -->
	        <batch:reader>
		  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
		  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
				<bean class="jobs.HVConsumersReader" scope="step">
					<constructor-arg index="0" type="int" value="0" />
			   		<!-- <constructor-arg index="1" type="java.lang.String" value="${AWS_SECRET_ACCESS_KEY}" /> -->

				  	<property name="dataSource" ref="db_connection" />
				  	<property name="rowMapper">
				    	<bean class="dao.mapping.HVConsumerRowMapper" />
				  	</property>
				  	<!-- <property name="sql">
				  		<value>
				    		SELECT ID,MSISDN,NAME,SEGMENT,LANGUAGE,BIRTH_DATE,BONUS,BONUS_EXPIRES_IN,LAST_UPDATE_TIME FROM HVC_BIRTHDAY_BONUS_MSISDN_EBA
				  		</value>
					</property> -->
				</bean>
	        </batch:reader>

	        <batch:processor>
	          <bean class="jobs.WishesNotificationProcessor" />
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.WishesNotificationWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>
	      </batch:chunk>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	    <batch:listener ref="jobRunListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job>

	<!-- To disable restart, set the attribute restartable to false on the job element:
	Remember that jobs are restartable by default. If you’re worried that you’ll forget that, set the restartable flag explicitly on all your jobs. -->
	<batch:job id="defaultBonusJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <!-- A single step cannot have both a "next" attribute and a transition element. -->
	  <!-- <batch:step id="stepA" next="stepB" /> -->
	  <batch:step id="defaultBonus">
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="3">
	      <!-- <batch:chunk reader="allHVCsWithNoBonusReader" processor="defaultBonusProcessor" writer="defaultBonusWriter" commit-interval="10" skip-limit="12" retry-limit="3"> -->
	      <!-- <batch:chunk reader="allHVConsumersWithNoBonusReader" commit-interval="10" skip-policy="skipPolicy" retry-limit="3"> -->
	      <batch:chunk reader="allHVConsumersWithNoBonusReader" commit-interval="10" skip-limit="12" retry-limit="3">

	        <batch:processor>
	          <bean class="jobs.DefaultBonusProcessor">
	          	<property name="dao" ref="dao" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.DefaultBonusWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- Each stream element involved in the step.
			By default, objects referenced using a reader, processor, and writer are automatically registered. You don’t need to specify them again here. -->
	        <!-- <batch:streams>
	          <batch:stream ref="allHVCsWithNoBonusReader" />
	          <batch:stream ref="defaultBonusWriter" />
	        </batch:streams> -->

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<!-- Skipping instead of failing -->
         	<!-- The job fails as soon as you reach 12 skipped products, as defined in the configuration -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />
            	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
			<!-- Automatic retry in a chunk-oriented step can make jobs more robust. It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
			You now know about the default retry configuration in Spring Batch, and this should be enough for most cases. -->
			<!-- Spring Batch only retries the item processing and item writing phases. By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
			You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
			
			Spring Batch retries only for exceptions thrown during item processing or item writing. Retry triggers a rollback, so retrying is costly : don’t abuse it!
			Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache. -->
			<!-- Override equals() and hashCode() when using retry
			In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
			By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
			It needs to track items closely to know which item could have triggered the retry.
			Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
			Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example. -->
	        <batch:retryable-exception-classes>
	          	<!-- <batch:include class="org.springframework.dao.OptimisticLockingFailureException" /> -->
	          	<!-- <batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" /> -->
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
	        </batch:retryable-exception-classes>

			<!-- Registers retry listener -->
			<!-- Spring Batch provides the RetryListener interface to react to any retried operation. A retry listener can be useful to log retried operations and to gather information.
			Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance). -->
			<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
			Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
        	<batch:retry-listeners>
          		<batch:listener ref="retryListener" />
        	</batch:retry-listeners>

			<!-- You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
			Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
			Use combined retry and skip when you don’t want a persisting transient error to fail a step. -->
	      </batch:chunk>

		  <!-- Avoiding a rollback for an exception -->
		  <!-- In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
		  This seems safe because an error could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
	      Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
		  This saves a rollback and therefore a new transaction. Having fewer transactions is better because transactions are costly.
		  Use the no-rollback-exception-classes feature only when you’re sure that an exception can’t corrupt a transaction; consider yourself warned! -->
		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
		  <!-- Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
			
		  When an item reader throws a skippable exception, Spring Batch just calls the read method again on the item reader to get the next item. There’s no rollback on the transaction.
			
		  When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
			
		  When the item writer throws a skippable exception, because Spring Batch doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
	      When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception. Spring Batch then rolls back the transaction and processes the chunk item by item.
		  Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
		  -->
	      <batch:no-rollback-exception-classes>
	      	 <batch:include class="java.lang.Exception" />
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	    <batch:listener ref="jobRunListener" />

			<!-- Configuring the CustomerItemListener. It is useful when Logging Invalid Records as an example -->
    	    <!-- Logging Invalid Records -->
    	    <!-- While skipping problematic records is a useful tool, by itself it can raise an issue. In some scenarios, the 
			ability to skip a record is okay. Say you are mining data and come across something you can’t resolve; it’s 
			probably okay to skip it. However, when you get into situations where money is involved, say when 
			processing transactions, just skipping a record probably will not be a robust enough solution. In cases 
			like these, it is helpful to be able to log the record that was the cause of the error. In this section, you will 
			look at using an ItemListener to record records that were invalid.  

			 <batch:skippable-exception-classes>
            	<batch:exclude class="java.lang.Exception" />
         	</batch:skippable-exception-classes>

			If you use the fixed length record job as an example and execute it with a file that contains an input 
			record longer than 63 characters (FlatFileItemReader example with a threshold FixedLengthTokenizer), an exception (FlatFileParseException) will be thrown. However, since you have configured your 
			job to skip all exceptions that extend Exception, the exception will not affect your job’s results, yet your 
			customerItemLogger will be called and log the item as required. -->
      		<batch:listener>
      			<!-- <bean id="customerItemLogger" class="jobs.listeners.HVConsumerItemListener" scope="step"> -->
      			<bean id="customerItemLogger" class="jobs.listeners.HVConsumerItemListener">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

    	<!-- Syntax for the step attribute on
    	Value/special character : String, Description : Exact value of the step exit status, Examples : COMPLETED,FAILED
    	Value/special character : *, Description : Matches 0 or more characters, Examples : * (matches any value) => COMPLETED* (matches COMPLETED and COMPLETED WITH SKIPS)
    	Value/special character : ?, Description : Matches exactly one character, Examples : C?T (matches CAT but not COUNT)

		Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
		This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific).

		WARNING : Be careful when transitioning to a step using the * special character. If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
		Perhaps this isn’t what you want; you may want to fail the job execution when a step fails. When using conditional transitions, you must handle failed steps yourself. -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->

    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="COMPLETED*" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->
	  </batch:step>
	</batch:job>

	<!-- Implementing a skip policy with no skip limit. Plugging in a skip policy in a chunk-oriented step -->
	<!-- Let’s say you know exactly on which exceptions you want to skip items, but you don’t care about the number of skipped items.
	You can implement your own skip policy, as shown in the following listing. -->
	<!-- <bean id="skipPolicy" class="exceptions.ExceptionSkipPolicy">                                       
	  	<constructor-arg value="org.springframework.batch.item.file.FlatFileParseException" />
  		<constructor-arg>
   			<list>
   				<value>org.springframework.batch.item.validator.ValidationException</value>
    			<value>exceptions.AirAvailabilityException</value>
   			</list>
  		</constructor-arg>
	</bean> -->

	<!-- Declares retry listener bean -->
	<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
	Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
	<bean id="retryListener" class="jobs.listeners.CustomRetryOperationsListener" />

	<!-- Job scope, introduced in Spring Batch 3.0 is similar to Step scope in configuration but is a Scope for the Job context so there is only one instance of such a bean per executing job.
	Additionally, support is provided for late binding of references accessible from the JobContext using #{..} placeholders. Using this feature, bean properties can be pulled from the job or job execution context and the job parameters. -->
	<!-- Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	Or by including a bean definition explicitly for the JobScope <bean class="org.springframework.batch.core.scope.JobScope" /> (but not both): -->
	<bean id="jobPhaseEventListener" class="jobs.listeners.JobPhaseEventListener" scope="job">
		<property name="productProperties" ref="productProperties" />

    	<!-- <property name="name" value="#{jobParameters['input']}" /> -->
    	<!-- <property name="name" value="#{jobExecutionContext['input.name']}.txt" /> -->
	</bean>

	<!-- Using a scope of Step is required in order to use late binding since the bean cannot actually be instantiated until the Step starts, which allows the attributes to be found.
	Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	or by including a bean definition explicitly for the StepScope <bean class="org.springframework.batch.core.scope.StepScope" /> (but not both): -->
    <bean id="jobRunListener" class="jobs.listeners.JobRunListener" scope="step">
      <property name="dao" ref="dao" />
      <property name="productProperties" ref="productProperties" />
	  
      <!-- <property name="resource" value="#{stepExecutionContext['input.file.name']}" />
      <property name="resource" value="#{jobExecutionContext['input.file.name']}" /> -->
    </bean>

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="allHVConsumersWithNoBonusReader" class="jobs.SynchronizingHVConsumerReader">
	  	<property name="delegate">
	  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
	  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
			<bean class="jobs.HVConsumersReader" scope="step">
				<constructor-arg index="0" type="int" value="2" />
		   		<!-- <constructor-arg index="1" type="java.lang.String" value="${AWS_SECRET_ACCESS_KEY}" /> -->

			  	<property name="dataSource" ref="db_connection" />
			  	<property name="saveState" value="false" />
			  	<property name="rowMapper">
			    	<bean class="dao.mapping.HVConsumerRowMapper" />
			  	</property>
			  	<!-- <property name="sql">
			  		<value>
			    		SELECT ID,MSISDN,NAME,SEGMENT,LANGUAGE,BIRTH_DATE,BONUS,BONUS_EXPIRES_IN,LAST_UPDATE_TIME FROM HVC_BIRTHDAY_BONUS_MSISDN_EBA WHERE BONUS = NULL
			  		</value>
				</property> -->
			</bean>
	  	</property>
	</bean>

	<!-- <bean id="reader" class="org.springframework.batch.item.database.JdbcCursorItemReader">
	  <property name="dataSource" ref="dataSource" />

	  <property name="sql" value="select id from product where update_timestamp &gt; ?" />
	  Assigns parameter to SQL query
	  <property name="preparedStatementSetter">
	    <bean class="org.springframework.jdbc.core.ArgumentPreparedStatementSetter" scope="step">
	      <constructor-arg value="#{jobParameters['updateTimestampBound']}" />
	    </bean>
	  </property>

	  <property name="rowMapper">
	  	Returns a String for each row
	    <bean class="org.springframework.jdbc.core.SingleColumnRowMapper">
	      <constructor-arg value="java.lang.String" />
	    </bean>
	  </property>
	</bean> -->

	<bean id="MultithreadedStepsTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor">
	  <property name="corePoolSize" value="03" />
	  <property name="maxPoolSize" value="03" />
	  <property name="queueCapacity" value="10" />
	</bean>

</beans>